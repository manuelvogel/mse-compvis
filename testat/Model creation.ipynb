{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86c4e80-14bf-408b-8a12-c5e59490c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f200229-f2c6-48ed-8771-980120311bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6047c1-09c7-4de3-b888-d0db27ef7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(training_data)\n",
    "train_size = int(len_data*0.8)\n",
    "test_size = len_data-train_size\n",
    "\n",
    "training_set, validation_set = random_split(training_data,[train_size,test_size ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38c2884-1cf2-493a-9fa7-8076efac6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(units)-2):\n",
    "            self.layers.append(torch.nn.Linear(units[i], units[i+1]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "        self.layers.append(torch.nn.Linear(units[len(units)-2], units[len(units)-1]))\n",
    "        print([layer for layer in self.layers])\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            *self.layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ea1857-eb1e-4459-bd8f-17b2b2536f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_set, validation_set):\n",
    "    # finally return the sequence of per epoch values\n",
    "    cost_hist = []\n",
    "    cost_hist_valid = []\n",
    "    acc_hist = []\n",
    "    acc_hist_valid = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # epoch: current epoch\n",
    "    # cost, cost_valid, acc, acc_valid: cost and acurracy (for training, validation set) per epoch     \n",
    "    \n",
    "    training_loader = DataLoader(training_set, batch_size=nbatch)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=nbatch)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        training_cost = 0\n",
    "        correct = 0\n",
    "        for inputs, targets in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            cost = cost_ce(predictions, targets)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            training_cost += cost.item()\n",
    "            correct += (torch.argmax(predictions, dim=1) == targets).sum()\n",
    "        \n",
    "        cost = training_cost / len(training_set)\n",
    "        acc = correct / len(training_set)\n",
    "\n",
    "        validation_cost = 0\n",
    "        correct = 0\n",
    "        for inputs, targets in validation_loader:\n",
    "            predictions = model(inputs)\n",
    "            cost = cost_ce(predictions, targets)\n",
    "            validation_cost += cost.item()\n",
    "            correct += (torch.argmax(predictions, dim=1) == targets).sum()\n",
    "\n",
    "        cost_valid = validation_cost / len(validation_set)\n",
    "        acc_valid = correct / len(validation_set)\n",
    "        \n",
    "        print(\"Epoch %i: %f, %f, %f, %f\"%(epoch, cost, acc, cost_valid, acc_valid))\n",
    "        \n",
    "        cost_hist.append(cost.data)\n",
    "        cost_hist_valid.append(cost_valid)\n",
    "        acc_hist.append(acc)\n",
    "        acc_hist_valid.append(acc_valid)\n",
    "    return cost_hist, cost_hist_valid, acc_hist, acc_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b2a10e-e5a2-4b88-ac0b-740e84fa02b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=784, out_features=100, bias=True), ReLU(), Linear(in_features=100, out_features=500, bias=True), ReLU(), Linear(in_features=500, out_features=10, bias=True)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#_,_, train_acc, val_acc = train_eval(model, lr, epochs, batch_size, training_set, validation_set)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 100, 500, 10])\n",
    "epochs = 10\n",
    "lr = 0.05\n",
    "batch_size = 64\n",
    "\n",
    "_,_, train_acc, val_acc = train_eval(model, lr, epochs, batch_size, training_set, validation_set)\n",
    "print(f'train_accuracy {train_acc[-1]}, val_accuracy {val_acc[-1]}')\n",
    "\n",
    "\n",
    "plt.plot(train_acc, 'r', label='Training loss')\n",
    "plt.plot(val_acc, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bafabf2-82a7-478b-b86b-65aaec65c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist-classifier.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
